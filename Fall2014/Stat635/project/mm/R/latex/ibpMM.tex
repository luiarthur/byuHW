\documentclass[mathserif]{article}
\usepackage{fullpage}
\usepackage{pgffor}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{verbatim}
\usepackage{appendix}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[UKenglish]{isodate} % for: \today
\cleanlookdateon                % for: \today
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\def\wl{\par \vspace{\baselineskip}\noindent}
\def\beginmyfig{\begin{figure}[htbp]\begin{center}}
\def\endmyfig{\end{center}\end{figure}}
\def\prodl#1#2#3{\prod\limits_{#1=#2}^{#3}}
\def\suml#1#2#3{\sum\limits_{#1=#2}^{#3}}
\def\ds{\displaystyle}
\def\tbf#1{\textbf{#1}}
\def\inv{^{\raisebox{.2ex}{$\scriptscriptstyle-1$}}}
\def\pm{^{\raisebox{.2ex}{$\scriptscriptstyle\prime$}}}
\newcommand{\m}[1]{\mathbf{\bm{#1}}} % Serif bold math

\begin{document}
% my title:
\begin{center}
  \section*{\textbf{Using the Indian Buffet Process to Estimate the Design Matrix Z in Random Intercept Mixed Models}
    \footnote{https://github.com/luiarthur/Fall2014/blob/master/Stat635/project}
  }
  \subsection*{\textbf{Arthur Lui}}
  \subsection*{\noindent\today}
\end{center}


\section*{Motivating Example}
Often, the data we observe are generated by latent features. Clustering
algorithms can be used to group these observations as a means of exploratory
analysis in preparation for further analysis. In Figure 1, we see a scatter
plot of a simulated dataset. The data were generated with a linear model with a
constant slope by three different intercepts. Hence, we observe three clusters
of data, each appear to have a linear trend, with the same slope, but different
intercepts.  Each cluster is composed of 30 points, and make up a total of 90
observations.  The figure in the middle is the true clustering of the data. The
right-most figure is the clustering we obtain using the k-means algorithm and
setting the number of clusters to three. We can see that the k-means algorithm
does very well in clustering the observations, but does not always classify
correctly. One reason is that no distribution is used to inform the k-means
algorithm, when there is actually a linear trend for each cluster. Using the
Indian buffet process (IBP) a Bayesian non-parametric distribution for binary
matrices of infinite dimensions, I will propose a model for obtaining the true
clustering of the data.

  \beginmyfig
    \includegraphics[scale=.3]{images/scatter.pdf}
    \includegraphics[scale=.3]{images/clus.pdf}
    \includegraphics[scale=.3]{images/kmean.pdf}
    \vspace{-5mm}
    \caption{Simulated Observations. Scatter plot (Left). True clustering (Middle). Clustering
             obtained using k-means (Right).}
  \endmyfig


\section*{Review of Mixed Models}
Linear mixed models (LMM) are used when researchers want to account for the
random effects that exists between groups of observations in a linear model.
Each group may be repeated mearuements on the same subject. An LMM can be
expressed in mathematical terms as
\[
  \m{y = X\beta + Z\gamma + \epsilon},
\]
where $\m{y}$ is a vector of responses, $\m{X}$ are the covariates, $\m{Z}$ is
the block design matrix, and $\m{\beta \text{ and } \gamma}$ are the coefficients for $\m{X}$ and $\m{Z}$ respectively.


\section*{The IBP}
One key problem in recovering the latent structure responsible for generating
observed data is determining the number of latent features. The Indian Buffet
process (IBP) provides a flexible distribution for sparse binary matrices with
infinite dimensions (i.e. finite number of rows, and infinite number of
columns).  When used as a prior distribution in a latent feature model, the IBP
can learn the number of latent features generating the observations because it
can draw binary matrices which have a potentially infinite number of columns.
We will use the IBP as a prior distribution in a Gaussian latent feature model
to recover the latent structures generating the observations.\\

\noindent
The IBP is a distribution for sparse binary matrices with a finite number of
rows and potentially an infinite number of columns. The process of generating a
realization from the IBP can be described by an analogy involving Indian buffet
restaurants.\\

\noindent
Let $Z$ be an $N \times \infty$ binary matrix. Each row in $Z$ represents a
customer who enters an Indian buffet and each column represents a dish in the
buffet. Customers enter the restaurant one after another. The first customer
samples an $r=$Poisson$(\alpha)$ number of dishes, where $\alpha > 0$ is a mass
parameter which influences the final number of sampled dishes. This is
indicated in by setting the first r columns of the first row in $Z$ to be $1$.
The other values in the row are set to $0$. Each subsequent customer samples
each previously sampled dish with probability proportional to its popularity.
That is, the next customer samples dish $k$ with probability $m_k/i$,
where $m_k$ is the number of customers that sampled dish $k$, and $i$ is the
current customer number (or row number in $Z$). Each customer also samples an
additional Poisson$(\alpha/i)$ number of new dishes. Once all the $N$ customers
have gone through this process, the resulting $Z$ matrix will be a draw from
the Indian buffet process with mass parameter $\alpha$. In other words, $Z \sim
\text{IBP}(\alpha)$. Note that $\alpha \propto K_+$, where $K_+$ is the final
number of sampled dishes (occupied columns). Figure 2.1 shows a draw from an
IBP(10) with 50 rows. The white squares are 1, indicating that a dish was
taken; black squares are 0, indicating that a dish was not taken. \\
\beginmyfig
  % Reverse Colors
  \includegraphics[scale=.4]{images/ibpExample.pdf}
  \caption{Two random draws from the Indian buffet process with 5 rows
           and $\alpha=2$ (Left), and $\alpha=5$ (Right). The expected number of new
           dishes for each customer, and consequently the number of non-zero columns in an
           IBP draw increases with $\alpha$.}
\endmyfig

\noindent
The probability of any particular matrix produced from this process is
\begin{equation}
  P(\bm{Z}) = \frac{\alpha^{K_+}}{\prodl{i}{1}{N} {K_1}^{(i)}!} 
              \exp\{-\alpha H_N\}\prodl{k}{1}{K_+}
              \frac{(N-m_k)!(m_k-1)!}{N!},
\end{equation}
where $H_N$ is the harmonic number, $\suml{i}{1}{N}\ds\frac{1}{i}$, $K_+$ is
the number of non-zero columns in $\bm Z$, $m_k$ is the $k^{th}$ column sum of
$\bm Z$, and $K_1^{(i)}$ is the ``number of new dishes" sampled by customer $i$.\\


\subsection*{Gibbs Sampler for Indian Buffet Process}
One way to get a draw from the IBP($\alpha$) is to simulate the process
according to the description above. Another way is to implement a Gibbs
sampler. We can implement a Gibbs sampler to draw from the IBP as follows:

\begin{enumerate}
  \item Start with an arbitrary binary matrix of $N$ rows
  \item For each row, $i$,
  \begin{enumerate}
    \item For each column, $k$,
    \item if $m_{-i,k}$ = $0$, delete column $k$. Otherwise,
    \item set $z_{ik}$ to $0$
    \item set $z_{ik}$ to $1$ with probability $P(z_{ik}=1|\bm{z_{-i,k}}) = \frac{m_{-i,k}}{i}$
    \item at the end of row $i$, add Poisson($\ds\frac{\alpha}{N}$) columns of $1$'s
  \end{enumerate}
  \item iterate step 2 a large number of times
\end{enumerate}
We can likewise incorporate this Gibbs sampler to sample from the posterior
distribution P($\bm{Z|X}$) where $\bm Z \sim \text{IBP}(\alpha)$ by
sampling from the complete conditional
\begin{equation}
  P(z_{ik}=1|\bm{Z_{-(ik)},X})  \propto p(\bm{X|Z}) P(z_{ik}=1|\bm{Z_{-(ik)}}).
\end{equation}

\noindent
The parameter $\alpha$ is often unknown, so it should be modeled. Note that the
conjugate prior for $\alpha$ is a Gamma distribution. Using a Gamma
distribution is appropriate since $\alpha$ is positive.
\[
  \begin{array}{rcl}
    \bm Z|\alpha & \sim & \text{IBP}(\alpha)\\
          \alpha & \sim & \text{Gamma}(a,b), \text{where $b$ is the scale parameter}\\
    & & \\
    p(\alpha|\bm Z) & \propto & p(\bm Z|\alpha) p(\alpha)\\
    p(\alpha|\bm Z) & \propto & \alpha^{K_+} e^{-\alpha H_N}  
                                \alpha^{a-1} e^{-\alpha/b}\\
    p(\alpha|\bm Z) & \propto & \alpha^{a+K_+-1} e^{-\alpha(1/b+H_N)}\\
  \end{array}
\]
\begin{equation}
  \alpha|\bm Z  \sim  \text{Gamma}(a+K_+, (1/b+H_N)^{-1})
\end{equation}

\section*{Using the IBP as a Prior for Z}
If we assume $\m{y}$ has a multivariate normal likelihood, then $\m{y \sim }$
MVN($\m{X\beta+Z\gamma},\sigma_\epsilon^2$). We can also give normal priors to
$\m{\beta}$ and $\m{\gamma}$. $\m{Z}$ will have an IBP($ \alpha$) prior. For
the hyper parameter $\alpha$, we can use a Gamma prior to make use of Gibbs
sampling. Succinctly,
\begin{align*}
  \m{y|X,\beta,Z,\gamma,}\sigma_\epsilon^2 &\sim \text{MVN}(\m{X\beta+Z\gamma},\sigma_\epsilon^2\m{I}_n)\\
  \m{Z}|\alpha &\sim \text{IBP}(\alpha)\\
  \alpha &\sim \text{Gamma}(1,1)\\
  \sigma_\epsilon^2 &\sim \text{Inverse-Gamma}(1,\text{rate}=5)\\
  \m{\beta} &\sim \text{MVN}(\m{0},10\m{I}_p)\\
  \m{\gamma} &\sim \text{MVN}(\m{0},10\m{I}_k)
\end{align*}

\noindent
Note that the dimensions of $\m{\gamma}$ depend on the dimensions of $\m{Z}$. So, we will integrate out 
$\gamma$. To further speed up computation, we will also integrate out $\beta$. Integrating out these
parameters is possible because we are using a gaussian likelihood and gaussian priors. Moreover, with
a likelihood that only depends on $\m{y,X}$,and $\m{Z}$, we can use equation (3) to implement a
Gibbs sampler to sample from the posterior of $\m{Z}$. Below, we show the marginalized likelihood that
we will use in our Gibbs sampler.
\begin{align*}
  \m{[y|Z,X]} &= \int_\beta\int_\gamma 
                   \m{[y|Z,X,\gamma,\beta][\gamma][\beta]}
                 d\gamma d\beta\\
              %&= \int_\beta \m{[\beta]} \left\{
              %     \int_\gamma
              %       \m{[y|Z,X,\gamma,\beta][\gamma]}
              %     d\gamma
              %   \right\}d\m{\beta}
              & \vdots \\
              &= (2\pi\sigma_\epsilon^2)^{-n/2} 
                 \left(\frac{\sigma_\epsilon^2}{\sigma_\gamma^2}\right)^{k/2}
                 \left(\frac{\sigma_\epsilon^2}{\sigma_\beta^2}\right)^{p/2}
                 \m{|M_1||M_2|}
                 \text{exp}\left(
                   -\frac{1}{2\sigma_\epsilon^2}\m{y^*}'\left(
                     \m{I}_n-\m{X^*M_2 X^*}
                   \right) \m{y^*}
                 \right),\\
  \text{where } \m{M_1} &= \left[\m{Z'}\m{Z} + \left(\frac{\sigma_\epsilon^2}{\sigma_\gamma^2}\right)\m{I}_k\right]\inv\\
                \m{U}   &= \m{I}_n - \m{ZM_1Z'}\\
                \m{y^*} &= \m{U^{1/2}y}\\
                \m{X^*} &= \m{U^{1/2}X}\\
                \m{M_2} &= \left[\m{X^*}'\m{X^*} + \left(\frac{\sigma_\epsilon^2}{\sigma_\beta^2}\right)\m{I}_d\right]\inv\\
\end{align*}

\section*{Results of Simulation Study}
Using a Gibbs sampler of 1000 iterations, we obtain draws from the posterior distributions for $\alpha$ and
$\sigma_\epsilon^2$. This information is shown in Figure 3.\\
\beginmyfig
  \includegraphics[scale=.7]{images/agpost.pdf}
  \caption{Posterior distribution for $\alpha$ and $\sigma_\epsilon^2$}
\endmyfig

\noindent
We desire an estimate for $\m{Z}$ based on its posterior distribution. We can obtain an 
estimate by summing the matrices drawn, using matrix addition, and then dividing the
each element of the matrix by the number of draws. We will then set a cell to ``1" if it
is greater than a threshold, and ``0" otherwise. Here, we choose a threshold of 0.5. 
We will call this estimate for $\m{Z}$ the posterior mean. In a random intercept model,
our $\m{Z}$ matrix has only one ``1" in any given row. Therefore, we will reparameterize
the posterior mean $\m{Z}$ such that each row has only one ``1". This is done by first
determining the number of unique rows in the posterior mean of $\m{Z}$. In this case, 
we have three unique rows -- (0,0), (1,1), and (1,0). So, we will set the rows with 
configutation (0,0) to (1,0,0), (1,1) to (0,1,0), and (1,0) to (0,0,1). This
reparameterization is shown in figure 4. Note that the algorithm predicted
each of the clusters with 100\% accuracy.\\
\beginmyfig
  \includegraphics[scale=.4]{images/EZpre.pdf}
  \includegraphics[scale=.4]{images/EZ.pdf}
  \vspace{-5mm}
  \caption{Estimated Z matrix (Left). Reparameterized estimated Z matrix (Right).}
\endmyfig  

\noindent
Using k-means, we can use the clustering labels to create a $\m{Z}$ matrix also to compare
our performance with the proposed model. The $\m{Z}$ matrix in Figure 5 represents the
clustering obtained from k-means. It is clear that the algorithm proposed out-performs
the k-means algorithm in obtaining the true clustering of our simulated data.
\beginmyfig
  \includegraphics[scale=.4]{images/KZ.pdf}
  \vspace{-5mm}
  \caption{Z Matrix (K-Means)}
\endmyfig  

\noindent
Based on our estimated $\m{Z}$ matrices, we can computed $\beta$ and $\gamma$ using 
standard LMM approaches. Figure 6 and Tables 1 and 2 summarize our findings. Our proposed
model estimates $\gamma$ very well, but is lacking in predicting $\beta$'s. I supposed
this is because our random effects were in reality not random, but fixed. Therefore,
by construction, we should be modelling with a fixed effect model, with an intercept for
each cluster in our design matrix. But, the notation we have used for our model resembles an
LMM.\\
\beginmyfig
  \includegraphics[scale=.3]{images/truemm.pdf}
  \includegraphics[scale=.3]{images/ibpmm.pdf}
  \includegraphics[scale=.3]{images/KMmm.pdf}
  \vspace{-5mm}
  \caption{Comparison of the LMM estimates using different $\m{Z}$ matrices}
\endmyfig

\begin{table}
  \centering
  \input{images/mb.tex}
  \vspace{-3mm}
  \caption{Estimates for $\beta$}
  \vspace{8mm}
  \input{images/mg.tex}
  \vspace{-3mm}
  \caption{Estimates for $\gamma$}
\end{table}

\section*{Future Work}
We have proposed a model to estimate the clusters based on their different
intercepts using a linear model to inform our algorithm. We simplified 
computation by using a gaussian likelihood and gaussian priors and integrating
out nuisance parameters. Of interest would be to substitute the
likelihood with a distribution from another member of the exponential
family, and using different priors. For instance, we could use
an exponential likelihood and gamma priors for the coefficients.
We would also like to develop a proposal mechanism so that
we can use standard MCMC algorithms, such as metropolis-hastings,
to obtain posterior estimates.

\end{document}
